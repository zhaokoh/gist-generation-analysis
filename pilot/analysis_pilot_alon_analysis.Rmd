---
title: "Comparison of descriptors between pilot data and Alon's experiment (V0)"
subtitle: "Pilot Analysis - `r format(Sys.time(), '%d %B, %Y')`"
geometry: left=2cm,right=2cm,top=2cm,bottom=2cm
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
header-includes: \usepackage{xcolor}
---

## Overview

This document describes the steps to analyse the descriptors between Zhao's pilot data (free-recall under fixed SOA) and Alon's experiment (no time constraint). 

## Materials/Data

* 20 images
* Descriptors from Alon's 640 experiment
* Translated descriptions of each image (4-5 participants) from Alon's experiment - ```translatedEXP5```
* Descriptors removed list from Alon's experiment
* Zhao's pilot data descriptors under 67, 133, 256ms

```{r include=FALSE}
source('embed_import_libraries.R')
source('embed_configuration.R')

# Functions
source('function-load_pilot_data.R')
source('function-plot_image_result.R')
source('function-plot_KL_divergence.R')
source('function-plot_word_cloud.R')
source('function-print_common_words.R')
```

```{r read summary df file, echo=FALSE, message=FALSE, include = FALSE, warning=FALSE}
all_df <- load_pilot_data(summary_file, details_file)
```

## Steps

1. Combine and pre-processed Zhao's pilot data, then calculate the weighted probabilities (by mean confidence)
1. Read Alon's descriptors and original subject descriptors and calculate the probability.
1. Merging the probabilities of Alon's descriptors with Zhao weighted probabilities (with confidence).

### Step 1: Combine and pre-processed Zhao's pilot data, then calculate the weighted probabilities (by mean confidence)

The figure below shows a sample of the data after the normalised weighted probabilities were derived. 

* ```probability``` of a word is the total occurrence of the word under an image per SOA.
* ```confidence``` of a word is the average participants' confidence ratings of that word under an image per SOA.
* $weighted\_prob = probability * confidence$
* $norm\_weighted\_prob = (probability * confidence)/sum(weighted\_prob)$ per image per SOA.


```{r, warning=FALSE, message=FALSE, echo=FALSE, results='markup'}
all_df <- all_df[, probability := frequency/sum(frequency), by = .(img, soa)]
all_df <- all_df[, weighted_prob := probability * confidence]
all_df <- all_df[, norm_weighted_prob := weighted_prob/sum(weighted_prob), by = .(img, soa)]
all_df <- all_df[, img_id := as.integer(gsub("im[0]+([1-9][0-9]*).jpg", "\\1", img))]

glimpse(all_df)
```
\hfill\break
Now, I checked the normalised weighted probabilities sum up to 1 for each image per soa.

Total unique combination of image and SOA is  
\hfill\break

```{r, warning=FALSE, message=FALSE, echo=TRUE, results='markup'}
print(count(unique(all_df[,.(img, soa)])))
```
\hfill\break

I expect the same if we sum up the normalised weighted probabilities.

```{r, warning=FALSE, message=FALSE, echo=TRUE, results='markup'}
prob_check_data <- all_df[, sum(norm_weighted_prob), by = .(img, soa)]
print(sum(prob_check_data$V1))
```
\hfill\break

Table below shows the top words for each image/soa that have the highest normalised weighted probability.
\hfill\break
```{r, warning=FALSE, message=FALSE, echo=FALSE, results='markup'}
max_prob_by_img_soa <- all_df[, max(norm_weighted_prob) , by = .(img, soa)]
setnames(max_prob_by_img_soa, c("img", "soa", "norm_weighted_prob"))
head(merge(all_df[,.(img, soa, word, norm_weighted_prob)], max_prob_by_img_soa, all = FALSE), 11)
```

### Step 2: Read Alon's descriptors and original subject descriptors and calculate the probability

* Source:
  * Alon's filtered descriptors
  * Alon's original translated descriptions from Shinji's (with max 5 translations per image)

```{r merge with alon data, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
alon_img_present_dt = fread(alon_present_words)
alon_img_descriptions_dt = fread(alon_subject_descriptions)

setnames(alon_img_descriptions_dt, c("img_id", "desc1", "desc2", "desc3", "desc4", "desc5"))

alon_filtered_img_present_dt = alon_img_present_dt[image_id %in% unique(all_df$img_id)]
alon_img_words <- separate_rows(alon_filtered_img_present_dt, present_words, convert = TRUE) %>%
  setnames(c("img_id", "word"))

alon_img_desc = data.table();

for (row_id in 1:nrow(alon_img_words)) {
  row <- alon_img_words[row_id, ]
  
  desc_row <- alon_img_descriptions_dt[img_id == row$img_id, ]
  expr <- paste("/ ", row$word, " | ",row$word, "|", row$word, " |", "^", row$word, "|", row$word, "$", "/", sep = "")
  sum <- 0
  
  for (desc in c(desc_row$desc1, desc_row$desc2, desc_row$desc3, desc_row$desc4, desc_row$desc5)) {
    pos <- gregexpr(expr, desc, ignore.case = TRUE)
    if (pos[[1]][1] != -1) {
      sum <- sum + 1
    }
  }
  
  r <- data.table(row$img_id, row$word, sum)
  alon_img_desc <- rbind(alon_img_desc, r)
}

setnames(alon_img_desc, c("img_id", "word", "frequency"))
alon_img_desc <- alon_img_desc[, probability := frequency/sum(frequency), by = img_id]
```

In Alon's dataset,

* ```frequency``` represents the number of occurrence of a word among the 4-5 translated image descriptions.
* ```probability``` is the normalised probability, i.e. $frequency/sum(frequency)$ - Sum of this column per image should be equals to 1.
```{r, warning=FALSE, echo = FALSE, results='markup'}
glimpse(alon_img_desc)
```

\hfill\break
Number of unique images:
```{r, warning=FALSE, echo = TRUE, results='markup'}
length(unique(alon_img_desc[, img_id]))
```

\hfill\break
I expect the sum of probabilities per image is 1, in another words, the sum of all probabilities should be equal to the number of unique images.
```{r, warning=FALSE, echo = TRUE, results='markup'}
sum(alon_img_desc[, sum(probability), by=img_id]$V1)
```

#### Removed words

The following words are in Alon's descriptors but unable to find in the translated descriptions, and also they are not part of the removed words.

```{r there are some words have 0 frequencies due to remove words, warning=FALSE, results='markup', echo=FALSE}
alon_remove_words_dt = fread(alon_remove_words)
alon_remove_words_list = unique(gsub("'", "", alon_remove_words_dt[,1]$V1))

not_removed_words_zero_freq <- alon_img_desc[frequency == 0][!word %in% alon_remove_words_list,]
print(not_removed_words_zero_freq)

alon_img_desc <- alon_img_desc[frequency != 0]
```
\hfill\break
I will remove all words that have zero frequency.

Here is a histogram that shows the number of word frequencies for Alon's descriptors in the images used in pilot.

Most of the descriptors used only once - among 4-5 descriptions. These could reflect the 2-rater words and 4-rater words that Alon used in the experiment.

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.show = 'asis', fig.align='left', results='markup'}
# Frequency table
alon_img_desc[, .(total = .N, percentage = (.N/nrow(alon_img_desc))*100), by=c("frequency")]

# Plot a distribution of words aggrements
ggplot(alon_img_desc, title="Frequency of each descriptor in 23 pilot images") + 
  geom_histogram(aes(x = frequency)) + 
  xlab("Frequency of word")

```

### Step 3: Merging the probabilities of Alon's descriptors with Zhao weighted probabilities (with confidence).

In this step, I merged the pilot dataset with Alon's in order to construct the word matrix with different SOAs.

In this data table:

* ```p_67``` is the probability (not normalised) for a word under SOA 67ms
* ```nwp_67``` is the normalised weighted probability for a word under SOA 67ms
* ```p_133``` is the probability (not normalised) for a word under SOA 133ms
* ```prob``` is the normalised probability from Alon's descriptors (no time limit)

```{r , warning=FALSE, message=FALSE, warning=FALSE, message=FALSE, echo=FALSE, results='markup'}
zhao_img_words_67 <- all_df[soa == 67, .(img_id, word, probability, norm_weighted_prob)]
zhao_img_words_133 <- all_df[soa == 133, .(img_id, word, probability, norm_weighted_prob)]
zhao_img_words_267 <- all_df[soa == 267, .(img_id, word, probability, norm_weighted_prob)]

summary_table <- merge(zhao_img_words_67, zhao_img_words_133, by = c("img_id", "word"), all = TRUE) %>%
  setnames(c("img_id", "word", "p_67", "nwp_67", "p_133", "nwp_133")) %>%
  merge(zhao_img_words_267, by = c("img_id", "word"), all = TRUE) %>%
  setnames(c("img_id", "word", "p_67", "nwp_67", "p_133", "nwp_133", "p_267", "nwp_267")) %>%
  merge(alon_img_desc[, c("img_id", "word", "probability")], by = c("img_id", "word"), all = TRUE) %>%
  setnames(c("img_id", "word", "p_67", "nwp_67", "p_133", "nwp_133", "p_267", "nwp_267", "prob")) %>%
  .[order(img_id, prob, word)]

glimpse(summary_table)
```

## Results

### Constructing KL Divergence Matrix

For the pilot data (67, 133, 267ms SOAs) and Alon's (Unlimited), I calculated the *pairwise* KL Divergence (using the normalised probabilities) and constructed a KL matrix.

The actual KL divergence (log2) is calculated using the following command:
```
KLMatrix[row, col] <- KL(rbind(prob[row,], prob[col,]), test.na = FALSE, unit="log2") # measure in bits
```

Note that KL divergence value is not symmetrical, i.e. $KL(P, Q)\ not\ equals\ to\ KL(Q, P)$.

### Aggregate the common words between two SOAs

Besides the KL Divergence Matrix, I have aggregated the common-word table that contains the columns:

* ```all_soas_alon``` - These are the words that are common between all SOAs in the pilot and Alon.
* ```all_soas``` - These are the words that are common between all SOAs in the pilot only.
* ```67_alon``` - These are the words that are common between 67ms in the pilot and Alon.
* ```133_alon``` - These are the words that are common between 133ms in the pilot and Alon.
* ```267_alon``` - These are the words that are common between 267ms in the pilot and Alon.

\hfill\break

```{r warning=FALSE, message = FALSE, include = FALSE, results="none", fig.show = 'none'}

#img_list = unique(summary_table$img_id)
kl <- plot_image_result(summary_table, 3)[, `:=`("img" = 3)]
kl <- merge(kl, plot_image_result(summary_table, 9)[, `:=`("img" = 9L)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 14)[, `:=`("img" = 14)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 104)[, `:=`("img" = 104)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 277)[, `:=`("img" = 277)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 321)[, `:=`("img" = 321)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 570)[, `:=`("img" = 570)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 774)[, `:=`("img" = 774)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 1078)[, `:=`("img" = 1078)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 1445)[, `:=`("img" = 1445)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 1684)[, `:=`("img" = 1684)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 1715)[, `:=`("img" = 1715)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 1758)[, `:=`("img" = 1758)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 2033)[, `:=`("img" = 2033)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 2164)[, `:=`("img" = 2164)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 2537)[, `:=`("img" = 2537)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 2667)[, `:=`("img" = 2667)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 2769)[, `:=`("img" = 2769)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 3279)[, `:=`("img" = 3279)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 6806)[, `:=`("img" = 6806)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 7360)[, `:=`("img" = 7360)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 8858)[, `:=`("img" = 8858)], all = TRUE)
kl <- merge(kl, plot_image_result(summary_table, 8876)[, `:=`("img" = 8876)], all = TRUE)

kl <- kl[Var1 != Var2]
kl <- kl[, `:=`(factor = paste0(Var1, '-', Var2))][order(factor)]
```
\hfill\break

#### Summary Statistics

A violin plot showing the mean KL divergence values across 23 images above.

```{r warning=FALSE, message = FALSE, results="asis", fig.show = 'asis', echo=FALSE}
ggplot(kl, aes(x = factor, y = value, color = img)) +   geom_violin(trim = FALSE) + 
  ggtitle("KL Divergence Values across SOAs (N=23 images)") +
  geom_jitter(shape=16, position=position_jitter(0)) + 
  stat_summary(fun.data=mean_sdl, mult=1, geom="pointrange", color="red") +
  xlab("Group") +
  ylab("KL Divergence (log2)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none") 
# +
#   geom_line(aes(group = img), color='grey')
# + 
#   geom_line(aes(group = img), color='black')

#}
```

### Results by Image

```{r warning=FALSE, message = FALSE, results="asis", fig.show = 'asis', echo=FALSE}

#img_list = unique(summary_table$img_id)
plot_image_result_no_return(summary_table, 3)
plot_image_result_no_return(summary_table, 9)
plot_image_result_no_return(summary_table, 14)
plot_image_result_no_return(summary_table, 104)
plot_image_result_no_return(summary_table, 277)
plot_image_result_no_return(summary_table, 321)
plot_image_result_no_return(summary_table, 570)
plot_image_result_no_return(summary_table, 774)
plot_image_result_no_return(summary_table, 1078)
plot_image_result_no_return(summary_table, 1445)
plot_image_result_no_return(summary_table, 1684)
plot_image_result_no_return(summary_table, 1715)
plot_image_result_no_return(summary_table, 1758)
plot_image_result_no_return(summary_table, 2033)
plot_image_result_no_return(summary_table, 2164)
plot_image_result_no_return(summary_table, 2537)
plot_image_result_no_return(summary_table, 2667)
plot_image_result_no_return(summary_table, 2769)
plot_image_result_no_return(summary_table, 3279)
plot_image_result_no_return(summary_table, 6806)
plot_image_result_no_return(summary_table, 7360)
plot_image_result_no_return(summary_table, 8858)
plot_image_result_no_return(summary_table, 8876)

